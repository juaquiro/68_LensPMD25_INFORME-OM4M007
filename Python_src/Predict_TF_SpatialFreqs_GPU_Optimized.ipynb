{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict_TF_SpatialFreqs â€” GPU-Optimized A/B and Per-Phase Timing\n",
    "\n",
    "This notebook compares your **legacy** NumPy-based pipeline with a **TensorFlow GPU-optimized** version.\n",
    "\n",
    "It reports per-phase timings for the GPU path (patch extraction, feature, predict) and total time for legacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, numpy as np, tensorflow as tf\n",
    "print('TF version:', tf.__version__)\n",
    "print('GPU devices:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "from ml_spatialfreq_utils import TrainedModelTF, synth_fringe\n",
    "import ml_spatialfreq_utils_v2 as v2\n",
    "#import ml_spatialfreq_utils_tf as tfutils  # from previous step (feature-only TF module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model + test image\n",
    "Adjust paths below to your saved model/scaler/meta. As a quick smoke test we generate a synthetic fringe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: synthetic image; replace with your real data\n",
    "NR, NC = 512, 511\n",
    "g = synth_fringe(NR, NC, w0_x=0.25, w0_y=0.12, modulation=1.0, background=0.0, noise_std=0.02)\n",
    "\n",
    "# Load your trained model\n",
    "# NOTE: set these paths to your files; TrainedModelTF expects model_path(+ optional scaler/meta)\n",
    "model_path = 'model.h5'           # TODO: point to your actual file\n",
    "scaler_path = None                # if you saved it\n",
    "meta_path = None                  # if you saved it (should include patch_NR, patch_NC, r, c, optional scaler stats)\n",
    "trained = TrainedModelTF(model_path=model_path, scaler_path=scaler_path, meta_path=meta_path)\n",
    "\n",
    "feature_name = 'feature_projected_DFT'  # or 'feature_DFT' / 'feature_GV'\n",
    "M_ROI = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a058a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resolve model paths from the DB Excel ---\n",
    "rootFolderDB = Path('../local_data/ML_Models')              # Adjust if needed\n",
    "db_name = 'DB-trainingSets-OM4M007.xlsx'        # Same as MATLAB\n",
    "db_sheet = 'Sheet1'                             \n",
    "trainingSet_Idx = 12                              # 1-based row index\n",
    "\n",
    "db_path = rootFolderDB / db_name\n",
    "print(\"Reading DB:\", db_path.resolve())\n",
    "db_tb = pd.read_excel(db_path, sheet_name=db_sheet)\n",
    "\n",
    "row_idx = trainingSet_Idx - 1\n",
    "assert 0 <= row_idx < len(db_tb), \"trainingSet_Idx out of range\"\n",
    "\n",
    "row = db_tb.iloc[row_idx].to_dict()\n",
    "\n",
    "# Expected columns for Python artifacts (preferred). If missing, you can fill them below.\n",
    "kerasModelPath = rootFolderDB / row.get('trainedModel')\n",
    "scalerPath     = rootFolderDB / row.get('scaler')\n",
    "featureMetadataPath  = rootFolderDB / row.get('feature_metadata')\n",
    "featureName    = row.get('featureName')\n",
    "patch_NR       = int(row.get('patch_NR'))\n",
    "patch_NC       = int(row.get('patch_NC'))\n",
    "\n",
    "print(\"kerasModel Name:\", kerasModelPath)\n",
    "print(\"scaler name    :\", scalerPath)\n",
    "print(\"feature_metadata name      :\", featureMetadataPath)\n",
    "print(\"feature Name   :\", featureName, \" patch:\", patch_NR, \"x\", patch_NC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run A/B comparison + timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wA, pxA, pyA, thA, QMA, wB, pxB, pyB, thB, QMB, tA, tB) = v2.benchmark_compare(g, trained, feature_name, M_ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect numerical differences (quick checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanrms(a,b):\n",
    "    m = ~np.isnan(a) & ~np.isnan(b)\n",
    "    if not np.any(m):\n",
    "        return np.nan\n",
    "    d = a[m] - b[m]\n",
    "    return float(np.sqrt(np.mean(d*d)))\n",
    "\n",
    "print('RMS diff w_phi:', nanrms(wA, wB))\n",
    "print('RMS diff phi_x:', nanrms(pxA, pxB))\n",
    "print('RMS diff phi_y:', nanrms(pyA, pyB))\n",
    "print('RMS diff theta:', nanrms(thA, thB))\n",
    "print('RMS diff QM   :', nanrms(QMA, QMB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Mixed precision\n",
    "If you want to experiment with mixed precision on GPUs like T4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print('Global policy set to:', mixed_precision.global_policy())\n",
    "# Re-run the A/B block above to observe the effect on timings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
